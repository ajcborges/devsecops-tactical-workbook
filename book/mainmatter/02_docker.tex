\chapter{Containers}

\includegraphics{../images/container-4203677_1920.jpg}

\justify
Containerization is the process of generating a fully functioning
software ecosystem that includes code and dependencies for part or all
of a project. The most popular and common tool for realizing
containerization is Docker. Using Docker, we can programatically build
an environment for our project, and pass the entirety of this
encapsulated environment from our local development machine, into the
Continuous Integration (CI) pipeline for testing, and eventually into
our production environment. Containerization helps us by offering a
consistent operating experience across disparate environments.

single: Continuous Integration single: Containerization single: Docker

\justify
It is more desirable to create projects that are built and replaced
frequently, than it is to attempt to upgrade and repair the
infrastructure, platforms, and project code. Attempts to patch and
upgrade project hosts "in place", such as with bare metal platforms for
example, quickly reveal great difficulty in maintaining consistency with
the project source. This also introduces issues keeping operating system
packages current, yet still compatible with the project. Imagine a
situation where an upgrade to a package is necessary to meet security
requirements, but this very same upgrade means the project stop working
since the package features have also changed. The result is most likely
angry end users and customers. Certainly not a situation we ever like to
find ourselves in.

\justify
There is obvious advantage of being able to quickly stand up new clones
of our project to replace existing instances that may be outdated,
insecure, etc. The idea of immutability\footnote{\url{https://www.hashicorp.com/resources/what-is-mutable-vs-immutable-infrastructure/}},
in reference to software projects, is the degree to which something, our
running project for example, can be changed. Immutability is desirable,
in that we wish to be able to simply replace outdated instances of our
project in their entirety. Upgrading and patching are inherently
problematic activities, high cost in terms of time, effort and money,
that we have the technology to dissociate from. With containerization,
we can more easily achieve immutability across the software lifecycle.

single: Immutability

\justify
Ephemerality is the concept of something being transitory in nature,
existing only briefly\footnote{\url{https://en.wikipedia.org/wiki/Ephemerality}}.
Using immutable containers makes it easier to realize infrastructure and
hosts that are ephemeral. Rather than spending a great deal of time
patching and upgrading one or more hosts as we might in a traditional
project stack that uses virtual machines or bare metal, we're going to
use Docker to create a new container in place of the old one. In other
words, we're running our project in containers that are immutable and
ephemeral to the degree possible.

single: Ephemerality

\justify
Docker images are "canned" (as in, prefabricated) or custom directives
for provisioning the operating system of a Docker container. One or more
images can be used as building blocks when configuring our containers.
For example, a base Linux image and base Python image might be combined
with our customizations that describe and point to our application code,
all of which make up a single containerized "server". We get the added
benefit of being able to switch quickly between base operating system
images with just a few lines of code change to our project. For example,
we could easily modify our container image to be predicated on Debian
rather than Red Hat distribution of Linux kernel and operating system
should the need arise.

\justify
See the Docker website for instructions on how to install and configure
Docker\footnote{\url{https://docs.docker.com/get-docker/}} . A properly
functioning Docker setup on your local machine is a requirement for the
exercises we will do later. Note that Podman is an acceptable substitute
for Docker, as detailed later in this chapter.

\justify
Once you have Docker installed and running on your workstation, take a
look at the two example files below. For now it's OK to see them and get
a general familiarity with their contents. Later we will use these files
to create containers for our projects.

\section{Dockerfile}
\justify
The Dockerfile is our basic unit of containerization. That is to say,
our containers, and the applications they contain, are defined by the
Dockerfile. This Dockerfile will dictate how we provision resources and
include operating system essentials and packages inside our container.
Each Dockerfile is predicated on a base image, such as Python/Debian 10
as shown in the example below.
\justify
Consider a directory named
\href{https://github.com/hotpeppersec/rapid_secdev_framework/tree/master/docker}{Docker}
and a file called
\href{https://github.com/hotpeppersec/rapid_secdev_framework/blob/master/docker/Dockerfile}{Dockerfile}
within this directory. Note the capitalization of the first letter in the file name. Some IDE's will key off this file and allow for additional syntax highlighting.

single: Dockerfile

\begin{lstlisting}[language=Python]
FROM python:3.9-buster
LABEL maintainer "Kevin Flynn <user@example.com>"

ENV DEBIAN_FRONTEND noninteractive}

ADD . /project
WORKDIR /project}

RUN apt update; \
apt -y install apt-utils
\end{lstlisting}
\justify
A valid Dockerfile begins with the \textbf{FROM} instruction. This
instruction specifies the base image that we will use to build our
project on. These base images come from the
\href{https://docs.docker.com/docker-hub/repos/}{Docker Hub
repositories}. We are setting an environment variable
\textbf{DEBIAN\_FRONEND} to the value of noninteractive, which will
cause the apt command to skip or ignore any interactive menus that are
encountered during execution of the apt command, since these would cause
our builds to "hang up" at an inaccessible interactive prompt. The
\textbf{ADD} and \textbf{WORKDIR} directives are meant to cause Docker
to use the /workdir directory as the root of the project "inside" the
container. Finally, we are directing Docker to \textbf{RUN} and apt
update and install the apt-utils package.

\subsection{docker-compose.yml}
\justify
The docker-compose tool and its associated docker-compose.yml file
allows us to manage multiple Docker containers for one or more
applications. We will add this file to our project to illustrate it's
composition and give ourselves the ability to extend our work later, as
needed.
\justify
A file called docker-compose.yml will exist alongside our Dockerfile in our docker directory.

single: docker-compose.yml

\begin{lstlisting}[language=Python]
version: '3'
services:
   devsecops:
      hostname: devsecops
      container_name: devsecops
   volumes:
      - ..:/project
   build:
      context: ..
      dockerfile: docker/Dockerfile
\end{lstlisting}
\justify
The docker-compose.yml file begins with a version specification. It's
important to note that the commands and structure of docker-compose.yml
can vary widely based on this version. While versions cannot be mixed,
all version are valid with respect to docker-compose itself. Wew specify
a service named "devsecops", and assign a host and container name. Under
"volumes" we are mounting the base of the project directory in the host
filesystem as "/project" in the container filesystem. The build
"directive" tells docker-compose how to locate the Dockerfile we wish to
use for the containers.

\subsection{Exercise: Testing Out Docker}
\justify
With Docker properly installed and an understanding of the necessary
configuration files, we can now try out our configuration. See
({myFig1}) for an illustration of how to lay out the project files in your local filesystem.

\digraph{docker}{
  size="8,4";
  node [fontname="Helvetica" fontsize=14 shape=box];
  edge [fontname="Symbol" fontsize=10];
  rapid_secdev_framework [shape=folder fontname="Symbol" label="rapid_secdev_framework"];
  docker [shape=folder fontname="Symbol" label="docker"];
  dockercom [fontname="Symbol" label="docker-compose.yml"];
  Dockerfile [fontname="Symbol" label="Dockerfile"];
  rapid_secdev_framework -> docker;
  docker -> dockercom;
  docker -> Dockerfile;
}

\clearpage
\justify
Here is a step by step description of how to prepare the creation of our first container:

\begin{itemize}
\item Create the "rapid\_secdev\_framework" folder.
  \begin{itemize}
  \item When creating folders, note that capitalization matters.
  \end{itemize}
\item In that folder, create another folder called "docker".
\item Now in the docker folder, create a text file with the name "Dockerfile".
  \begin{itemize}
  \item
    Copy and paste the example Dockerfile from earlier in this chapter
    into your text file.
  \end{itemize}
\item Also in the docker folder, create a text file with the name "docker-compose.yml"
  \begin{itemize}
  \item Copy and paste the example docker-compose.yml file from earlier in this chapter into your second text file.
  \end{itemize}
\end{itemize}

\justify
Here is an example of the BASH shell commands you can use to accomplish
the steps of the exercise. You can substitue vi for your favorite text
editor as needed. Note that typing the "docker-compose" command on line
6 will reference the devsecops "service" we specified on line 3 of the
docker-compose.yml file.

\begin{lstlisting}[language=Python]
mkdir rapid_secdev_framework
cd rapid_secdev_framework
mkdir docker
vi docker/Dockerfile}
vi docker/docker-compose.yml
\end{lstlisting}
\justify
With our files created and populated, we are ready to generate our
container based on our specified configuration.

\begin{lstlisting}[language=Python]
docker-compose -f docker/docker-compose.yml build devsecops
\end{lstlisting}
\justify
If all went well, you should now have a shell prompt from "inside" the
new container. Recall that we set our \textbf{WORKDIR} variable to /project in the Dockerfile. Following that example, we now have Dockerfile and docker-compose.yml in the directory /project/docker, having mounted the project directory from the host machine "inside" the container.

\subsubsection{Testing from GitHub}
\justify
It should be noted that the link to the GitHub repository for the accompanying project for this book is
\url{https://github.com/hotpeppersec/rapid_secdev_framework}. In the next chapter we will explore how to "clone" the project repository and do our work directory from there.

\section{Substituting Podman for Docker}
\justify
Podman is an Open Source container engine from the Open Containers Initiative (OCI). The Podman service is purportedly capable of being a drop-in replacement for Docker, although it only runs on Linux hosts at the time of this writing. Podman gives the user the ability to use traditional Docker commands, without the need to run a daemon to do so,
as is the case with Docker.
\href{https://developers.redhat.com/blog/2019/02/21/podman-and-buildah-for-docker-users/}{According to William Henry of Red Hat Inc} , the Podman approach is simply to directly interact with the image registry, with the container and image storage, and with the Linux kernel through the runC container runtime process (rather than with a daemon).

single: Podman single: Open Containers Initiative (OCI)
\justify
You can install Podman by \href{https://podman.io/getting-started/installation.html}{following the instructions} at their web site.
\justify
Here is the change for the unprivileged\_userns\_clone error:

\begin{lstlisting}[language=Python]
user@devsecops:~$ podman
cannot clone: Operation not permitted
user namespaces are not enabled in /proc/sys/kernel/unprivileged_userns_clone
Error: could not get runtime: cannot re-exec process
user@devsecops:~$ sudo sysctl kernel.unprivileged_userns_clone=1
user@devsecops:~$ podman-v
podman version 1.9.1
\end{lstlisting}
\justify
Once Podman is installed properly you should be able to
alias docker=podman and use it as a drop in replacement for docker.

\section{Container Orchestration}
\justify
An orchestrator for containers can be thought of as an engine which
allows for their provisioning, deployment, scaling, monitoring, load
balancing, and more. The Container Orchestrator is meant to manage the
lifecycle and visibility of a container at all stages.
\justify
Kubernetes is an example, perhaps the penultimate example, of a
Container Orchestrator. Folks throughout the DevSecOps, Software and
Security communities are using Kubernetes these days, and with good
reason. It's adoption as a means to manage and replicate containers, and
scale the applications they contain, has been nothing short of
revolutionary. System administrators and developers can do more, better
work. Granted, this comes at the expense of introduction yet another
framework to learn, and and no small amount of complexity.

single: Kubernetes single: Orchestration
\justify
An orchestrator helps us achieve immutability, and scale to meet user demand quickly and easily by abstracting away concerns that come with operating workloads in a bare metal or VM environment.
\justify
Kubernetes and other orchestrators are rapidly evolving. To ignore this game-changing ecosystem is to be left behind in terms of technological prowess. That said, it's just beyond the scope of this book. Learning about containers, pipelines, infrastructure, and so on are the foundational elements you will want to become familiar with in preparation for expanding your mindset into the greater dimensionality that orchestration realizes.
\justify
For this stage of our journey to DevSecOps enlightenment, it is enough to know that orchestration exists and have a bit of familiarity with its purpose.