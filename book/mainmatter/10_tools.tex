\chapter{Tools}

\includegraphics{../images/school-tools-3596680_1920.jpg}

We can reduce our Mean Time To Deploy (MTTD)\footnote{\url{https://www.packer.io/downloads/}}
by using tools to prepare and generate our machine images
programatically, and with scripting languages such as HCL, which
Terraform\footnote{\url{https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2\#Images:sort=name}}
is based on. In this section we examine these tools in greater depth.

single: MTTD single: Terraform

\begin{figure}
   \centering
   \includegraphics{../images/infra_flow.png}
   \caption{The pipeline flow.}
\end{figure}

Consider the following diagram {myFig4} as we discuss the tools we'll
use to implement our infrastructure as coode and associated
configurations in the cloud provider network.

\hypertarget{packer}{%
   \subsection{Packer}\label{packer}}

Using Hashicorp Packer is a great way to nail down the contents of a
machine image before we bring up an instance. Download Packer from the
Hashicorp web site in preparation for the follwing steps\footnote{\url{https://github.com/bonclay7/aws-amicleaner}}
. We will focus on creating images for our cloud provider from the
command line. Bear in mind it is also possible to use Terraform to
manage the creation of Packer generated machine images. Generating
machine images on the fly using Terraform would increase our degree of
ephepmerality and immutability.

single: Packer

\hypertarget{packer-example-configuration-for-aws}{%
   \subsubsection{Packer Example Configuration for
      AWS}\label{packer-example-configuration-for-aws}}

Here is an example of how to set up a JSON file to build a Packer image
in AWS. Save the contents of this file into
`packer/aws-debian-host.json`:

\begin{Shaded}
   \begin{Highlighting}[]
      \KeywordTok{\{}
      \StringTok{"variables"}\NormalTok{: }\KeywordTok{\{}
      \StringTok{"aws_access_key"}\NormalTok{: }\StringTok{""}\NormalTok{,}
      \StringTok{"aws_secret_key"}\NormalTok{: }\StringTok{""}
      \KeywordTok{\}}\NormalTok{,}
      \StringTok{"builders"}\NormalTok{:}\BuiltInTok{ [}
      \NormalTok{   \{}
      \StringTok{"type"}\NormalTok{: }\StringTok{"amazon-ebs"}\NormalTok{,}
      \StringTok{"access_key"}\NormalTok{: }\StringTok{"\{\{user }\KeywordTok{`}\ExtensionTok{aws_access_key}\KeywordTok{`}\StringTok{\}\}"}\NormalTok{,}
      \StringTok{"secret_key"}\NormalTok{: }\StringTok{"\{\{user }\KeywordTok{`}\ExtensionTok{aws_secret_key}\KeywordTok{`}\StringTok{\}\}"}\NormalTok{,}
      \StringTok{"region"}\NormalTok{: }\StringTok{"us-west-2"}\NormalTok{,}
      \StringTok{"source_ami_filter"}\NormalTok{: \{}
      \StringTok{"filters"}\NormalTok{: \{}
      \StringTok{"virtualization-type"}\NormalTok{: }\StringTok{"hvm"}\NormalTok{,}
      \StringTok{"name"}\NormalTok{: }\StringTok{"ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*"}\NormalTok{,}
      \StringTok{"root-device-type"}\NormalTok{: }\StringTok{"ebs"}
      \NormalTok{      \},}
      \StringTok{"owners"}\NormalTok{: [}
      \StringTok{"099720109477"}
      \NormalTok{      ],}
      \StringTok{"most_recent"}\NormalTok{: true}
      \NormalTok{      \},}
      \StringTok{"instance_type"}\NormalTok{: }\StringTok{"t2.micro"}\NormalTok{,}
      \StringTok{"ssh_username"}\NormalTok{: }\StringTok{"ubuntu"}\NormalTok{,}
      \StringTok{"ami_name"}\NormalTok{: }\StringTok{"generic-lab-host \{\{timestamp\}\}"}\NormalTok{,}
      \StringTok{"tags"}\NormalTok{: \{}
      \StringTok{"Name"}\NormalTok{: }\StringTok{"Packer-Ansible"}\NormalTok{,}
      \StringTok{"Project"}\NormalTok{: }\StringTok{"DevSecOps"}\NormalTok{,}
      \StringTok{"Commit"}\NormalTok{: }\StringTok{"unknown"}
      \NormalTok{      \}}
      \NormalTok{   \}}
      \NormalTok{],}
      \StringTok{"provisioners"}\NormalTok{: [}
      \NormalTok{   \{}
      \StringTok{"type"}\NormalTok{: }\StringTok{"shell"}\NormalTok{,}
      \StringTok{"execute_command"}\NormalTok{: }\StringTok{"echo 'packer' | sudo -S sh -c '\{\{ .Vars \}\} \{\{ .Path \}\}'"}\NormalTok{,}
      \StringTok{"inline"}\NormalTok{: [}
      \StringTok{"mkdir -p /home/secdevops/.ssh"}\NormalTok{,}
      \StringTok{"chmod 700 /home/secdevops/.ssh"}\NormalTok{,}
      \StringTok{"cd /home/secdevops && git clone https://github.com/wwce/ctf_scoreboard.git"}\NormalTok{,}
      \StringTok{"cp /home/secdevops/rapid_secdev_framework/packer/authorized_keys /home/secdevops/.ssh"}\NormalTok{,}
      \StringTok{"chown -R ubuntu /home/ubuntu"}\NormalTok{,}
      \StringTok{"cp /home/ubuntu/rapid_secdev_framework/packer/ctfd.service /etc/systemd/system"}
      \NormalTok{   ]\}}
      \BuiltInTok{]}
      \KeywordTok{\}}
   \end{Highlighting}
\end{Shaded}

\hypertarget{packer-example-configuration-for-gcp}{%
   \subsubsection{Packer Example Configuration for
      GCP}\label{packer-example-configuration-for-gcp}}

Here is an example of how to set up a JSON file to build a Packer image
in Google Compute. Save the contents of this file into
`packer/gcp-debian-host.json`:

\begin{Shaded}
   \begin{Highlighting}[]
      \KeywordTok{\{}
      \StringTok{"builders"}\NormalTok{:}\BuiltInTok{ [}
      \NormalTok{      \{}
      \StringTok{"type"}\NormalTok{: }\StringTok{"googlecompute"}\NormalTok{,}
      \StringTok{"account_file"}\NormalTok{: }\StringTok{"/home/secdevops/.config/gcloud/my-gcloud-creds-file.json"}\NormalTok{,}
      \StringTok{"project_id"}\NormalTok{: }\StringTok{"sec-dev-ops-000378"}\NormalTok{,}
      \StringTok{"source_image_family"}\NormalTok{: }\StringTok{"debian-10"}\NormalTok{,}
      \StringTok{"zone"}\NormalTok{: }\StringTok{"us-central1-a"}\NormalTok{,}
      \StringTok{"image_description"}\NormalTok{: }\StringTok{"SecDevOps Debian Host"}\NormalTok{,}
      \StringTok{"image_name"}\NormalTok{: }\StringTok{"generic-lab-host"}\NormalTok{,}
      \StringTok{"ssh_username"}\NormalTok{: }\StringTok{"root"}\NormalTok{,}
      \StringTok{"metadata"}\NormalTok{: \{ }\StringTok{"enable-oslogin"}\NormalTok{: }\StringTok{"false"}\NormalTok{ \}}
      \NormalTok{      \}}
      \NormalTok{   ],}
      \StringTok{"provisioners"}\NormalTok{: [}
      \NormalTok{      \{}
      \StringTok{"type"}\NormalTok{: }\StringTok{"shell"}\NormalTok{,}
      \StringTok{"inline"}\NormalTok{: [}
      \StringTok{"sleep 10"}\NormalTok{,}
      \StringTok{"mkdir -p /home/secdevops/.ssh"}\NormalTok{,}
      \StringTok{"chmod 700 /home/secdevops/.ssh"}
      \NormalTok{         ]}
      \NormalTok{      \}}
      \NormalTok{   ]}
      \NormalTok{\}}
   \end{Highlighting}
\end{Shaded}

\hypertarget{validating-packer-json-files}{%
   \subsubsection{Validating Packer JSON
      Files}\label{validating-packer-json-files}}

Once the JSON files are created and saved in the packer directory, we
can use the packer tool to validate them. Type
packer validate \textless{}filename\textgreater{} to validate each new
JSON file. This gives you a chance to find and fix any errors before the
next step, the build phase.

Note that your validation commands may fail if the cloud provider
credentials have not been configured at this point.

\hypertarget{building-images-with-packer}{%
   \subsubsection{Building Images with
      Packer}\label{building-images-with-packer}}

Finally, we are ready to build our new images. Try typing
packer build \textless{}filename\textgreater{} to create the image. You
shoudl see output similar to the following, but with a unique AMI ID.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{Build} \StringTok{'amazon-ebs'}\NormalTok{ finished.}

      \NormalTok{==}\OperatorTok{>} \ExtensionTok{Builds}\NormalTok{ finished. The artifacts of successful builds are:}
      \ExtensionTok{--}\OperatorTok{>}\NormalTok{ amazon-ebs: AMIs were created:}
      \ExtensionTok{us-west-2}\NormalTok{: ami-0e9e6427509a9d0b5}
   \end{Highlighting}
\end{Shaded}

The AMI ID "ami-0e9e6427509a9d0b5" is now a usable image that we can
include in our Terraform builds.

\hypertarget{removing-packer-images-from-cloud-provider}{%
   \subsubsection{Removing Packer Images from Cloud
      Provider}\label{removing-packer-images-from-cloud-provider}}

You may want to remove the images from AWS/GCP since storing them incurs
additional cost, whether they are in use or not\footnote{\url{https://registry.terraform.io/modules/trussworks/lambda-packerjanitor/aws/1.0.0}}
.

To remove stale machine images from AWS, you may try a tool such as
aws-amicleaner\footnote{\url{https://www.terraform.io/intro/index.html}}
, which is available to be installed via Python/pip as well as from the
GitHub repository for the project.

Another AWS specfic tool is "lambda-packerjanitor" from Trusworks.

\hypertarget{terraform}{%
   \subsection{Terraform}\label{terraform}}

Terraform, created by Hashicorp in 2014, is a tool for building,
changing, and versioning infrastructure safely and efficiently . Install
the latest version of Terraform in preparation for the activities that
follow.

single: Terraform

Consider the relevant Terraform files that we will include in our
projects.

\begin{description}
   \item[digraph folders \{]
         "aws" {[}shape=folder{]}; "main.tf" {[}shape=rect{]}; "output.tf"
         {[}shape=rect{]}; "terraform.tfvars" {[}shape=rect{]}; "variables.tf"
         {[}shape=rect{]}; "aws" -\textgreater{} "main.tf"; "aws" -\textgreater{}
         "output.tf"; "aws" -\textgreater{} "terraform.tfvars"; "aws"
         -\textgreater{} "variables.tf";
\end{description}

\}

\hypertarget{terraform.tfvars}{%
   \subsubsection{terraform.tfvars}\label{terraform.tfvars}}

When working with AWS as cloud provider, life gets a bit easier if you
save a copy of your console credentials in a file called
terraform.tfvars as seen in the next example. You must be very careful
not to commit these credentials to GitHub! Adding the line
terraform.tfvars to your .gitignore file at the top level of your lab
repository helps a lot. Keeping track of your credentials is very
important!

single: terraform.tfvars

An example of a local terraform.tfvars file follows. Remember that this
file will never be checked into GitHub or any other revision control
toolset.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{aws_access_key}\NormalTok{ = AKIAJCQ6WHUXVOKZ8RQQ}
      \ExtensionTok{aws_secret_key}\NormalTok{ = q27qR8fwdHLUh7WOEH3JVd2VHjfRlQs1jlhhbZbQ}
   \end{Highlighting}
\end{Shaded}

\hypertarget{main.tf}{%
   \subsubsection{main.tf}\label{main.tf}}

This file will contain the bulk of our Terraform configurations. As with
Python, we have the ability to reference modules, both internal and
exteral. The main.tf file is the place the module references are made.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{module} \StringTok{"security_group"}\NormalTok{ \{}
      \BuiltInTok{source}\NormalTok{  = }\StringTok{"terraform-aws-modules/security-group/aws"}
      \ExtensionTok{version}\NormalTok{ = }\StringTok{"~> 3.0"}

      \ExtensionTok{name}\NormalTok{        = }\StringTok{"DevSecOps"}
      \ExtensionTok{description}\NormalTok{ = }\StringTok{"Security group for the cloud lab"}
      \ExtensionTok{vpc_id}\NormalTok{      = data.aws_vpc.default.id}

      \ExtensionTok{ingress_cidr_blocks}\NormalTok{ = [}\StringTok{"0.0.0.0/0"}\NormalTok{]}
      \ExtensionTok{ingress_rules}\NormalTok{       = [}\StringTok{"http-80-tcp"}\NormalTok{, }\StringTok{"all-icmp"}\NormalTok{, }\StringTok{"ssh-tcp"}\NormalTok{]}
      \ExtensionTok{egress_rules}\NormalTok{        = [}\StringTok{"all-all"}\NormalTok{]}
      \NormalTok{\}}
   \end{Highlighting}
\end{Shaded}

We can also designate our data sources in the main.tf file. Consider the
following Terraform data sources. These AWS data sources reference our
Virtual Private Cloud (VPC) and provider-assigned IPv4 Subnets.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{data} \StringTok{"aws_vpc"} \StringTok{"default"}\NormalTok{ \{}
      \ExtensionTok{default}\NormalTok{ = true}
      \NormalTok{\}}

      \ExtensionTok{data} \StringTok{"aws_subnet_ids"} \StringTok{"all"}\NormalTok{ \{}
      \ExtensionTok{vpc_id}\NormalTok{ = data.aws_vpc.default.id}
      \NormalTok{\}}
   \end{Highlighting}
\end{Shaded}

single: main.tf

\hypertarget{outputs.tf}{%
   \subsubsection{outputs.tf}\label{outputs.tf}}

We can display or export the resources we've created in main.tf using a
file known as outputs.tf. We may have a need to display the IP address
of host instances we've just created, which is helpful to a user who
needs to log in. We may also wish to make values available to other
Terraform modules.

Consider the following output declarations from our example code.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{output} \StringTok{"web_public_ip"}\NormalTok{ \{}
      \ExtensionTok{description}\NormalTok{ = }\StringTok{"Public IPs assigned to the web instance"}
      \ExtensionTok{value}\NormalTok{       = aws_instance.web.public_ip}
      \NormalTok{\}}

      \ExtensionTok{output} \StringTok{"kali_public_ip"}\NormalTok{ \{}
      \ExtensionTok{description}\NormalTok{ = }\StringTok{"Public IPs assigned to the kali instance"}
      \ExtensionTok{value}\NormalTok{       = aws_instance.kali.public_ip}
      \NormalTok{\}}
   \end{Highlighting}
\end{Shaded}

single: outputs.tf

\hypertarget{variables.tf}{%
   \subsubsection{variables.tf}\label{variables.tf}}

The variables.tf file is another common file seen in projects in AWS,
GCP and other cloud providers. It contains declarations of variables,
and often values for variables as well, that will be used in the main.tf
file. As an example there might be region information or even the name
of the image we created previously with Packer.

single: variables.tf

Consider the following example. Here we declare a "region" variable in
the file variables.tf.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{variable} \StringTok{"region"}\NormalTok{ \{}
      \ExtensionTok{description}\NormalTok{ = }\StringTok{"AWS region to launch servers."}
      \ExtensionTok{default}\NormalTok{     = }\StringTok{"us-west-2"}
      \NormalTok{\}}
   \end{Highlighting}
\end{Shaded}

\hypertarget{verification}{%
   \subsubsection{Verification}\label{verification}}

Terraform has some commands, validate and fmt (short for "format") that
we can use to syntactically verify our configuration before sending it
off to the cloud provider to act upon. Validating your Terraform files
is as easy as typing terraform validate in the directory the files exist
in.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{user@devsecops}\NormalTok{::~/workspace/rapid_secdev_framework/aws$ terraform validate}
      \ExtensionTok{Success}\NormalTok{! The configuration is valid.}
   \end{Highlighting}
\end{Shaded}

To get your Terrform files into a clean standard format, the
terraform fmt command works well. There is also the option to do this
formatting from inside the VSCode window on a per-file basis.

\hypertarget{plan}{%
   \subsubsection{Plan}\label{plan}}

First we will will create a "plan" in preparation for application.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{user@devsecops}\NormalTok{::~/workspace/rapid_secdev_framework/aws$ terraform plan -out franklin.out}
      \ExtensionTok{Refreshing}\NormalTok{ Terraform state in-memory prior to plan...}
      \ExtensionTok{The}\NormalTok{ refreshed state will be used to calculate this plan, but will not be}
      \ExtensionTok{persisted}\NormalTok{ to local or remote state storage.}

      \ExtensionTok{data.aws_vpc.default}\NormalTok{: Refreshing state...}
      \ExtensionTok{data.aws_subnet_ids.all}\NormalTok{: Refreshing state...}

      \ExtensionTok{------------------------------------------------------------------------}

      \ExtensionTok{An}\NormalTok{ execution plan has been generated and is shown below.}
      \ExtensionTok{Resource}\NormalTok{ actions are indicated with the following symbols:}
      \ExtensionTok{+}\NormalTok{ create}


      \ExtensionTok{Plan}\NormalTok{: 8 to add, 0 to change, 0 to destroy.}

      \ExtensionTok{------------------------------------------------------------------------}

      \ExtensionTok{This}\NormalTok{ plan was saved to: franklin.out}

      \ExtensionTok{To}\NormalTok{ perform exactly these actions, run the following command to apply:}
      \ExtensionTok{terraform}\NormalTok{ apply }\StringTok{"franklin.out"}
   \end{Highlighting}
\end{Shaded}

\hypertarget{apply}{%
   \subsubsection{Apply}\label{apply}}

The apply action is where the rubber meets the proverbial road. This
action will transmit our configurations to the cloud provider and
allocate the necessary resources to stand up our environment.

With our plan in place, we can now "apply" that plan to the cloud
provider. This can take a counsiderable amount of time, depending on the
complexity of the desired configuration. Note that Terraform will prompt
you to enter "yes" before it will proceed.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{user@devsecops}\NormalTok{::~/workspace/rapid_secdev_framework/aws$ terraform apply}
      \ExtensionTok{data.aws_vpc.default}\NormalTok{: Refreshing state...}
      \ExtensionTok{data.aws_subnet_ids.all}\NormalTok{: Refreshing state...}

      \ExtensionTok{An}\NormalTok{ execution plan has been generated and is shown below.}
      \ExtensionTok{Resource}\NormalTok{ actions are indicated with the following symbols:}
      \ExtensionTok{+}\NormalTok{ create}

      \ExtensionTok{Plan}\NormalTok{: 8 to add, 0 to change, 0 to destroy.}

      \ExtensionTok{Do}\NormalTok{ you want to perform these actions?}
      \ExtensionTok{Terraform}\NormalTok{ will perform the actions described above.}
      \ExtensionTok{Only} \StringTok{'yes'}\NormalTok{ will be accepted to approve.}

      \ExtensionTok{Enter}\NormalTok{ a value: yes}

      \ExtensionTok{Apply}\NormalTok{ complete! Resources: 8 added, 0 changed, 0 destroyed.}

      \ExtensionTok{Outputs}\NormalTok{:}

      \ExtensionTok{kali_public_ip}\NormalTok{ = 34.221.121.11}
      \ExtensionTok{web_public_ip}\NormalTok{ = 54.186.129.232}
   \end{Highlighting}
\end{Shaded}

\hypertarget{ansible}{%
   \subsection{Ansible}\label{ansible}}

Environments where you have a set of repeatable configuration steps can
be deployed more quickly with Ansible. Building a set of good Ansible
playbooks over team means you can pick and choose the most useful
patterns in future projects. A true force multiplier.

single: Ansible

\hypertarget{installing-ansible}{%
   \subsubsection{Installing Ansible}\label{installing-ansible}}

Simply adding "ansible" to python/requirements.txt will make Ansible
available in our Docker containers. Now when we type make docker, pip
will take care of the installation for us. Then we can experiment with
Ansible playbook runs.

\hypertarget{ansible-playbooks}{%
   \subsubsection{Ansible Playbooks}\label{ansible-playbooks}}

Ansible breaks down it's execution runs into discrete workflows known as
playbooks. Playbooks are executed on the target hosts to implement
configurations. It's quite useful to be able to kick off a playbook run
on the taget host every 15 minutes. This is a direct example of
Continuous Deployment in action. If somethings changes in the GitHub
repository, we want that to propagate out to the targets and the latest
configuration to be applied to the server. We can also deploy a newer
version of an application and then stop and start the applicationto
effect the change.

Ansible playbooks break down target hosts into groupings known as roles.

\hypertarget{testing-ansible-playbooks}{%
   \subsubsection{Testing Ansible
      Playbooks}\label{testing-ansible-playbooks}}

There is a test framework known as "molecule" that can be used to test
ansible playbooks.

single: Molecule

\begin{Shaded}
   \begin{Highlighting}[]
      \NormalTok{$ }\ExtensionTok{molecule}\NormalTok{ init role -r logfwd}
      \ExtensionTok{--}\OperatorTok{>}\NormalTok{ Initializing new role logfwd...}
      \ExtensionTok{Initialized}\NormalTok{ role in /ansible/roles/logfwd successfully.}
   \end{Highlighting}
\end{Shaded}

\hypertarget{ansible-vault}{%
   \subsection{Ansible Vault}\label{ansible-vault}}

Vault is a tool that is included with Ansible. You may notice that
ansible-vault is a symlink back to ansible on your system. Vault is an
easy way to protect secrets using AES-256 encryption in your GitHub
repositories. For example, we can use it to secure data at rest in a
repository, or protect system configuration data as it transits through
our pipelines out to our cloud providers.

single: AES-256 single: Vault

\hypertarget{encrypting-a-file-with-vault}{%
   \subsubsection{Encrypting a File with
      Vault}\label{encrypting-a-file-with-vault}}

single: encryption

Let's try encrypting a file... using another file! Create a text file
with some random contents. For example, create a file in your home
directory called my\_dog.txt with the following contents:

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{My}\NormalTok{ dog has fleas.}
   \end{Highlighting}
\end{Shaded}

Now we can encrypt some data using this file as the encryption key. For
the sake of example, let's assume we have a file called
data\_to\_protect.txt that we would like to encrypt.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{ansible-vault}\NormalTok{ encrypt --vault-password-file ~/my_dog.txt data_to_protect.txt}
   \end{Highlighting}
\end{Shaded}

Now when we view the data\_to\_protect.txt file, we can see it has been
encrypted and appears as a long series of seemingly nonsense characters.

\hypertarget{decrypting-a-file-with-vault}{%
   \subsubsection{Decrypting a File with
      Vault}\label{decrypting-a-file-with-vault}}

single: decryption

At some point, we are going to want to decrypt our data so it becomes
usable, we can perform operations on it, and so on. As long as we keep
or recreate the original key file on our host, or create an identical
copy of the key file some some target/remote host, we will be able to
decrypt the data. This is quite useful to us indeed, when it comes to
protecting our data.

\begin{Shaded}
   \begin{Highlighting}[]
      \ExtensionTok{ansible-vault}\NormalTok{ decrypt --vault-password-file ~/my_dog.txt data_to_protect.txt}
   \end{Highlighting}
\end{Shaded}

\clearpage

\hypertarget{tool-directory-structure}{%
   \subsection{Tool Directory Structure}\label{tool-directory-structure}}

Files and folders relevant to this chapter are organized as shown below.

\begin{description}
   \item[digraph folders \{]
         "devsecops" {[}shape=folder{]}; "ansible" {[}shape=folder{]}; "aws"
         {[}shape=folder{]}; "packer" {[}shape=folder{]}; "aws-debian-host.json"
         {[}shape=rect{]}; "gcp-debian-host.json" {[}shape=rect{]}; "main.tf"
         {[}shape=rect{]}; "outputs.tf" {[}shape=rect{]}; "terraform.tfvars"
         {[}shape=rect{]}; "variables.tf" {[}shape=rect{]}; "devsecops"
         -\textgreater{} "ansible"; "devsecops" -\textgreater{} "aws";
         "devsecops" -\textgreater{} "packer"; "aws" -\textgreater{} "main.tf";
         "aws" -\textgreater{} "outputs.tf"; "aws" -\textgreater{}
         "terraform.tfvars"; "aws" -\textgreater{} "variables.tf"; "packer"
         -\textgreater{} "aws-debian-host.json"; "packer" -\textgreater{}
         "gcp-debian-host.json";
\end{description}

\}
